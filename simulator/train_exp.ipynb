{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sac_tf import SAC__Agent\n",
    "from env_wrappers import *\n",
    "from gym_car_intersect.envs import CarRacingHackatonContinuous2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    '''\n",
    "    Class to hold agent, environment and replay buffer. \n",
    "    Also it is a place to controll hyperparameters of learning process.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, batch_size=32, hidden_size=256, buffer_size=10 * 1000):\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # for reward history \n",
    "        self.update_steps_count = 0\n",
    "        self.game_count = 0\n",
    "        self.history = []\n",
    "        \n",
    "        # init replay buffer\n",
    "        self.cur_write_index = 0\n",
    "        self.buffer_size = buffer_size\n",
    "        self.full_buf_size = 0\n",
    "        self.buffer = [\n",
    "            # state\n",
    "            [],\n",
    "            # action\n",
    "            [],\n",
    "            # reward\n",
    "            [],\n",
    "            # new state\n",
    "            [],\n",
    "            # done\n",
    "            [],\n",
    "        ]\n",
    "        \n",
    "        # init environment and agent\n",
    "        env = CarRacingHackatonContinuous2(num_bots=0, start_file=None, is_discrete=True)\n",
    "        env = chainerrl.wrappers.ContinuingTimeLimit(env, max_episode_steps=1000)\n",
    "        env = MaxAndSkipEnv(env, skip=4)\n",
    "        env = DiscreteWrapper(env)\n",
    "        env = WarpFrame(env)\n",
    "        self.env = env\n",
    "        \n",
    "        self.agent = SAC__Agent(\n",
    "            picture_shape=(84, 84, 3), \n",
    "            extra_size=12, \n",
    "            action_size=5, \n",
    "            hidden_size=hidden_size\n",
    "        )\n",
    "        self.reset_env()\n",
    "        \n",
    "    def reset_env(self, inc_counter=True):\n",
    "        self.env.reset()\n",
    "        if inc_counter:\n",
    "            self.game_count += 1\n",
    "        \n",
    "        \n",
    "    def insert_N_sample_to_replay_memory(self, N, temperature=0.5):\n",
    "        for _ in range(N):\n",
    "            \n",
    "            state_pic, state_extra = self.env.state[0], np.array(self.env.state[1])\n",
    "            action = self.agent.get_single_action(\n",
    "                state,\n",
    "                need_argmax=False,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "            new_state_pic, new_state_extra, reward, done, info = self.env.step(np.argmax(action))\n",
    "            \n",
    "            # state\n",
    "            self.buffer[0][self.cur_write_index] = state_pic\n",
    "            self.buffer[1][self.cur_write_index] = state_extra\n",
    "            # action\n",
    "            self.buffer[2][self.cur_write_index] = action\n",
    "            # reward\n",
    "            self.buffer[3][self.cur_write_index] = np.array([reward])\n",
    "            # new state\n",
    "            self.buffer[4][self.cur_write_index] = new_state_pic\n",
    "            self.buffer[5][self.cur_write_index] = np.array(new_state_extra)\n",
    "            # done flag\n",
    "            self.buffer[6][self.cur_write_index] = 1.0 if done else 0.0\n",
    "            self.cur_write_index += 1\n",
    "            if self.cur_write_index >= self.buffer_size:\n",
    "                self.cur_write_index =  0\n",
    "            \n",
    "            if self.full_buf_size < self.buffer_size:\n",
    "                self.full_buf_size += 1\n",
    "            \n",
    "            # reset env if done\n",
    "            if done:\n",
    "                self.reset_env()\n",
    "                \n",
    "                \n",
    "    def iterate_over_buffer(self, steps):\n",
    "        cur_steps = 0\n",
    "        is_break = False\n",
    "        while True:\n",
    "            indexes = np.arange(self.full_buf_size)\n",
    "            np.random.shuffle(indexes)\n",
    "            \n",
    "            for ind in range(0, len(indexes), self.batch_size):\n",
    "                yield (\n",
    "                    self.buffer[i][indexes[ind : ind + self.batch_size]]\n",
    "                    for i in range(5)\n",
    "                )\n",
    "                cur_steps += 1\n",
    "                if cur_steps >= steps:\n",
    "                    is_break = True\n",
    "                    break\n",
    "            if is_break:\n",
    "                break\n",
    "    \n",
    "    def update_agent(\n",
    "            self, \n",
    "            update_step_num=500,\n",
    "            temperature=0.5,\n",
    "            gamma=0.7,\n",
    "            v_exp_smooth_factor=0.8,\n",
    "            need_update_VSmooth=False\n",
    "    ):\n",
    "        for batch in self.iterate_over_buffer(update_step_num):\n",
    "            self.update_steps_count += 1\n",
    "            self.agent.update_step(\n",
    "                batch, \n",
    "                temperature=temperature, \n",
    "                gamma=gamma,\n",
    "                v_exp_smooth_factor=v_exp_smooth_factor,\n",
    "                need_update_VSmooth=need_update_VSmooth,\n",
    "            )\n",
    "            \n",
    "    def iterate_over_test_game(self, max_steps=1000):\n",
    "        self.reset_env(inc_counter=True)\n",
    "        was_game_finit = False\n",
    "        for _ in range(max_steps):\n",
    "            state = np.hstack([self.env.state[1], self.goal])\n",
    "            action = self.agent.get_single_action(\n",
    "                state,\n",
    "                need_argmax=False,\n",
    "                temperature=1,\n",
    "            )\n",
    "            new_state, reward, done, info = self.env.step(np.argmax(action))\n",
    "            \n",
    "            yield self.env.state[0], action, reward, done\n",
    "            \n",
    "            if done:\n",
    "                was_game_finit = True\n",
    "                break\n",
    "        return None, None, was_game_finit\n",
    "            \n",
    "    def get_test_game_total_reward(\n",
    "            self, \n",
    "            max_steps=1000, \n",
    "            temperature=10,\n",
    "            add_to_memory=True,\n",
    "    ):\n",
    "        total_reward = 0\n",
    "        was_game_finit = False\n",
    "        \n",
    "        for _, _, reward, done in self.iterate_over_test_game(max_steps=1000):\n",
    "            if done:\n",
    "                break\n",
    "            total_reward += reward\n",
    "\n",
    "            \n",
    "        if add_to_memory:\n",
    "            self.history.append([self.game_count, total_reward])\n",
    "        \n",
    "        return total_reward\n",
    "    \n",
    "    \n",
    "    def get_test_game_mean_reward(\n",
    "        self,\n",
    "        n_games=10,\n",
    "        max_steps=1000, \n",
    "        temperature=10,\n",
    "        add_to_memory=True\n",
    "    ):\n",
    "        sm = 0\n",
    "        for _ in range(n_games):\n",
    "            sm += self.get_test_game_total_reward(max_steps, temperature, add_to_memory=False)\n",
    "        sm /= n_games\n",
    "        \n",
    "        if add_to_memory:            \n",
    "            self.history.append([self.game_count, sm])\n",
    "            \n",
    "        return sm\n",
    "    \n",
    "    def get_history(self):\n",
    "        return np.array(self.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-636031ea71db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-03f7776b8082>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_size, hidden_size, buffer_size)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         )\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_counter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-03f7776b8082>\u001b[0m in \u001b[0;36mreset_env\u001b[0;34m(self, inc_counter)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_counter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minc_counter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mmd_default/lib/python3.7/site-packages/gym/core.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cars/simulator/env_wrappers.py\u001b[0m in \u001b[0;36mobservation\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;31m# frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         frame = cv2.resize(np.float32(frame) / 255, (self.width, self.height),\n\u001b[0m\u001b[1;32m    223\u001b[0m                            interpolation=cv2.INTER_AREA)\n\u001b[1;32m    224\u001b[0m         \u001b[0;31m# print(\"frame_shape\", frame.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "holder = Holder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
