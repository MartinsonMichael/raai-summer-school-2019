{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sac_tf import SAC__Agent\n",
    "from env_wrappers import *\n",
    "from gym_car_intersect.envs import CarRacingHackatonContinuous2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import display, HTML\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from multiprocessing import Process\n",
    "\n",
    "plt.rcParams['animation.ffmpeg_path'] = u'/home/mmartinson/.conda/envs/mmd_default/bin/ffmpeg'\n",
    "\n",
    "def plot_sequence_images(image_array, need_disaply=True, need_save=True):\n",
    "    ''' Display images sequence as an animation in jupyter notebook\n",
    "    \n",
    "    Args:\n",
    "        image_array(numpy.ndarray): image_array.shape equal to (num_images, height, width, num_channels)\n",
    "    '''\n",
    "    dpi = 72.0\n",
    "    xpixels, ypixels = image_array[0].shape[:2]\n",
    "    fig = plt.figure(figsize=(ypixels/dpi, xpixels/dpi), dpi=dpi)\n",
    "    im = plt.figimage(image_array[0])\n",
    "\n",
    "    def animate(i):\n",
    "        im.set_array(image_array[i])\n",
    "        return (im,)\n",
    "\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, \n",
    "        animate,\n",
    "        frames=len(image_array), \n",
    "        interval=33, \n",
    "        repeat_delay=1, \n",
    "        repeat=True\n",
    "    )\n",
    "    if need_save:\n",
    "        import os\n",
    "        if not os.path.exists('save_animation_folder'):\n",
    "            os.makedirs('save_animation_folder')\n",
    "        anim.save(f'./save_animation_folder/{datetime.datetime.now()}.mp4')\n",
    "    if need_disaply:\n",
    "        display(HTML(anim.to_html5_video()))\n",
    "        \n",
    "def visualize(agent_holder):\n",
    "    ims = []\n",
    "    for state, action, reward, done in agent_holder.iterate_over_test_game(max_steps=2500):\n",
    "        if done:\n",
    "            break\n",
    "        ims.append(agent_holder.env.state)\n",
    "    return np.array(ims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    '''\n",
    "    Class to hold agent, environment and replay buffer. \n",
    "    Also it is a place to controll hyperparameters of learning process.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, batch_size=32, hidden_size=256, buffer_size=10 * 1000):\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # for reward history \n",
    "        self.update_steps_count = 0\n",
    "        self.game_count = 0\n",
    "        self.history = []\n",
    "        \n",
    "        # init replay buffer\n",
    "        self.cur_write_index = 0\n",
    "        self.buffer_size = buffer_size\n",
    "        self.full_buf_size = 0\n",
    "        self.buffer = [\n",
    "            # state\n",
    "            [],\n",
    "            # action\n",
    "            [],\n",
    "            # reward\n",
    "            [],\n",
    "            # new state\n",
    "            [],\n",
    "            # done\n",
    "            [],\n",
    "        ]\n",
    "        \n",
    "        # init environment and agent\n",
    "        env = CarRacingHackatonContinuous2(num_bots=0, start_file=None, is_discrete=True)\n",
    "        env = chainerrl.wrappers.ContinuingTimeLimit(env, max_episode_steps=1000)\n",
    "        env = MaxAndSkipEnv(env, skip=4)\n",
    "#         env = DiscreteWrapper(env)\n",
    "        env = WarpFrame(env, channel_order='hwc')\n",
    "        self.env = env\n",
    "        \n",
    "        self.agent = SAC__Agent(\n",
    "            picture_shape=(84, 84, 3), \n",
    "            extra_size=12, \n",
    "            action_size=5, \n",
    "            hidden_size=hidden_size\n",
    "        )\n",
    "        self.env_state = None\n",
    "        self.reset_env()\n",
    "        \n",
    "    def reset_env(self, inc_counter=True):\n",
    "        self.env_state = self.env.reset()\n",
    "        if inc_counter:\n",
    "            self.game_count += 1\n",
    "        \n",
    "        \n",
    "    def insert_N_sample_to_replay_memory(self, N, temperature=0.5):\n",
    "        for _ in range(N):\n",
    "            \n",
    "            if self.env_state is None:\n",
    "                self.reset_env()\n",
    "            \n",
    "            action = self.agent.get_single_action(\n",
    "                self.env_state,\n",
    "                need_argmax=False,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "            new_state, reward, done, info = self.env.step(np.argmax(action))\n",
    "            \n",
    "            if len(self.buffer[0]) <= self.cur_write_index:\n",
    "                for i in range(5):\n",
    "                    self.buffer[i].append(None)\n",
    "            # state\n",
    "            self.buffer[0][self.cur_write_index] = self.env_state\n",
    "            # action\n",
    "            self.buffer[1][self.cur_write_index] = action\n",
    "            # reward\n",
    "            self.buffer[2][self.cur_write_index] = np.array([reward])\n",
    "            # new state\n",
    "            self.buffer[3][self.cur_write_index] = new_state\n",
    "            # done flag\n",
    "            self.buffer[4][self.cur_write_index] = 1.0 if done else 0.0\n",
    "            self.env_state = new_state\n",
    "            \n",
    "            self.cur_write_index += 1\n",
    "            if self.cur_write_index >= self.buffer_size:\n",
    "                self.cur_write_index =  0\n",
    "            \n",
    "            if self.full_buf_size < self.buffer_size:\n",
    "                self.full_buf_size += 1\n",
    "            \n",
    "            # reset env if done\n",
    "            if done:\n",
    "                self.reset_env()\n",
    "                \n",
    "                \n",
    "    def iterate_over_buffer(self, steps):\n",
    "        cur_steps = 0\n",
    "        is_break = False\n",
    "        buffer = [np.array(x) for x in self.buffer]\n",
    "        while True:\n",
    "            indexes = np.arange(self.full_buf_size)\n",
    "            np.random.shuffle(indexes)\n",
    "            \n",
    "            for ind in range(0, len(indexes), self.batch_size):\n",
    "                yield (\n",
    "                    buffer[i][indexes[ind : ind + self.batch_size]]\n",
    "                    for i in range(5)\n",
    "                )\n",
    "                cur_steps += 1\n",
    "                if cur_steps >= steps:\n",
    "                    is_break = True\n",
    "                    break\n",
    "            if is_break:\n",
    "                break\n",
    "    \n",
    "    def update_agent(\n",
    "            self, \n",
    "            update_step_num=500,\n",
    "            temperature=0.5,\n",
    "            gamma=0.7,\n",
    "            v_exp_smooth_factor=0.8,\n",
    "            need_update_VSmooth=False\n",
    "    ):\n",
    "        for batch in self.iterate_over_buffer(update_step_num):\n",
    "            self.update_steps_count += 1\n",
    "            self.agent.update_step(\n",
    "                batch, \n",
    "                temperature=temperature, \n",
    "                gamma=gamma,\n",
    "                v_exp_smooth_factor=v_exp_smooth_factor,\n",
    "                need_update_VSmooth=need_update_VSmooth,\n",
    "            )\n",
    "            \n",
    "    def iterate_over_test_game(self, max_steps=1000):\n",
    "        self.reset_env(inc_counter=True)\n",
    "        was_game_finit = False\n",
    "        for _ in range(max_steps):\n",
    "            action = self.agent.get_single_action(\n",
    "                self.env_state,\n",
    "                need_argmax=False,\n",
    "                temperature=1,\n",
    "            )\n",
    "            self.env_state, reward, done, info = self.env.step(np.argmax(action))\n",
    "            \n",
    "            yield self.env_state, action, reward, done\n",
    "            \n",
    "            if done:\n",
    "                was_game_finit = True\n",
    "                break\n",
    "        return None, None, was_game_finit\n",
    "            \n",
    "    def get_test_game_total_reward(\n",
    "            self, \n",
    "            max_steps=1000, \n",
    "            temperature=10,\n",
    "            add_to_memory=True,\n",
    "    ):\n",
    "        total_reward = 0\n",
    "        was_game_finit = False\n",
    "        \n",
    "        for _, _, reward, done in self.iterate_over_test_game(max_steps=1000):\n",
    "            if done:\n",
    "                break\n",
    "            total_reward += reward\n",
    "\n",
    "            \n",
    "        if add_to_memory:\n",
    "            self.history.append([self.game_count, total_reward])\n",
    "        \n",
    "        return total_reward\n",
    "    \n",
    "    \n",
    "    def get_test_game_mean_reward(\n",
    "        self,\n",
    "        n_games=10,\n",
    "        max_steps=1000, \n",
    "        temperature=10,\n",
    "        add_to_memory=True\n",
    "    ):\n",
    "        sm = 0\n",
    "        for _ in range(n_games):\n",
    "            sm += self.get_test_game_total_reward(max_steps, temperature, add_to_memory=False)\n",
    "        sm /= n_games\n",
    "        \n",
    "        if add_to_memory:            \n",
    "            self.history.append([self.game_count, sm])\n",
    "            \n",
    "        return sm\n",
    "    \n",
    "    def get_history(self):\n",
    "        return np.array(self.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "holder = Holder(\n",
    "    batch_size=64,\n",
    "    hidden_size=256,\n",
    "    buffer_size=5 * 10**3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1) \n",
    "\n",
    "holder.insert_N_sample_to_replay_memory(1000)\n",
    "\n",
    "for i in range(10 * 1000):\n",
    "    gamma = min(0.9, 0.1 + i / 5000)\n",
    "    temperature = min(1.5, 0.2 + i / 1000)\n",
    "    \n",
    "    holder.insert_N_sample_to_replay_memory(1000, temperature=temperature - 0.1)\n",
    "    holder.update_agent(update_step_num=100, temperature=temperature, gamma=gamma)\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        holder.get_test_game_mean_reward()\n",
    "\n",
    "        clear_output(wait = True)\n",
    "        ax.plot(holder.get_history()[:, 0], holder.get_history()[:, 1])\n",
    "        display(fig)\n",
    "        plt.pause(0.5)\n",
    "        \n",
    "    if i % 100 == 99:\n",
    "        ims = visualize(holder)\n",
    "        Process(target=plot_sequence_images, args=(ims, False, True)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
