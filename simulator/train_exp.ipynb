{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sac_tf import SAC__Agent\n",
    "from env_wrappers import *\n",
    "from gym_car_intersect.envs import CarRacingHackatonContinuous2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    '''\n",
    "    Class to hold agent, environment and replay buffer. \n",
    "    Also it is a place to controll hyperparameters of learning process.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, batch_size=32, hidden_size=256, buffer_size=10 * 1000):\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # for reward history \n",
    "        self.update_steps_count = 0\n",
    "        self.game_count = 0\n",
    "        self.history = []\n",
    "        \n",
    "        # init replay buffer\n",
    "        self.cur_write_index = 0\n",
    "        self.buffer_size = buffer_size\n",
    "        self.full_buf_size = 0\n",
    "        self.buffer = [\n",
    "            # state\n",
    "            [],\n",
    "            # action\n",
    "            [],\n",
    "            # reward\n",
    "            [],\n",
    "            # new state\n",
    "            [],\n",
    "            # done\n",
    "            [],\n",
    "        ]\n",
    "        \n",
    "        # init environment and agent\n",
    "        env = CarRacingHackatonContinuous2(num_bots=0, start_file=None, is_discrete=True)\n",
    "        env = chainerrl.wrappers.ContinuingTimeLimit(env, max_episode_steps=1000)\n",
    "        env = MaxAndSkipEnv(env, skip=4)\n",
    "#         env = DiscreteWrapper(env)\n",
    "        env = WarpFrame(env, channel_order='hwc')\n",
    "        self.env = env\n",
    "        \n",
    "        self.agent = SAC__Agent(\n",
    "            picture_shape=(84, 84, 3), \n",
    "            extra_size=12, \n",
    "            action_size=5, \n",
    "            hidden_size=hidden_size\n",
    "        )\n",
    "        self.env_state = None\n",
    "        self.reset_env()\n",
    "        \n",
    "    def reset_env(self, inc_counter=True):\n",
    "        self.env_state = self.env.reset()\n",
    "        if inc_counter:\n",
    "            self.game_count += 1\n",
    "        \n",
    "        \n",
    "    def insert_N_sample_to_replay_memory(self, N, temperature=0.5):\n",
    "        for _ in range(N):\n",
    "            \n",
    "            if self.env_state is None:\n",
    "                self.reset_env()\n",
    "            \n",
    "            action = self.agent.get_single_action(\n",
    "                self.env_state,\n",
    "                need_argmax=False,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "            new_state, reward, done, info = self.env.step(np.argmax(action))\n",
    "            \n",
    "            if len(self.buffer[0]) <= self.cur_write_index:\n",
    "                for i in range(5):\n",
    "                    self.buffer[i].append(None)\n",
    "            # state\n",
    "            self.buffer[0][self.cur_write_index] = self.env_state\n",
    "            # action\n",
    "            self.buffer[1][self.cur_write_index] = action\n",
    "            # reward\n",
    "            self.buffer[2][self.cur_write_index] = np.array([reward])\n",
    "            # new state\n",
    "            self.buffer[3][self.cur_write_index] = new_state\n",
    "            # done flag\n",
    "            self.buffer[4][self.cur_write_index] = 1.0 if done else 0.0\n",
    "            self.env_state = new_state\n",
    "            \n",
    "            self.cur_write_index += 1\n",
    "            if self.cur_write_index >= self.buffer_size:\n",
    "                self.cur_write_index =  0\n",
    "            \n",
    "            if self.full_buf_size < self.buffer_size:\n",
    "                self.full_buf_size += 1\n",
    "            \n",
    "            # reset env if done\n",
    "            if done:\n",
    "                self.reset_env()\n",
    "                \n",
    "                \n",
    "    def iterate_over_buffer(self, steps):\n",
    "        cur_steps = 0\n",
    "        is_break = False\n",
    "        buffer = [np.array(x) for x in self.buffer]\n",
    "        while True:\n",
    "            indexes = np.arange(self.full_buf_size)\n",
    "            np.random.shuffle(indexes)\n",
    "            \n",
    "            for ind in range(0, len(indexes), self.batch_size):\n",
    "                yield (\n",
    "                    buffer[i][indexes[ind : ind + self.batch_size]]\n",
    "                    for i in range(5)\n",
    "                )\n",
    "                cur_steps += 1\n",
    "                if cur_steps >= steps:\n",
    "                    is_break = True\n",
    "                    break\n",
    "            if is_break:\n",
    "                break\n",
    "    \n",
    "    def update_agent(\n",
    "            self, \n",
    "            update_step_num=500,\n",
    "            temperature=0.5,\n",
    "            gamma=0.7,\n",
    "            v_exp_smooth_factor=0.8,\n",
    "            need_update_VSmooth=False\n",
    "    ):\n",
    "        for batch in self.iterate_over_buffer(update_step_num):\n",
    "            self.update_steps_count += 1\n",
    "            self.agent.update_step(\n",
    "                batch, \n",
    "                temperature=temperature, \n",
    "                gamma=gamma,\n",
    "                v_exp_smooth_factor=v_exp_smooth_factor,\n",
    "                need_update_VSmooth=need_update_VSmooth,\n",
    "            )\n",
    "            \n",
    "    def iterate_over_test_game(self, max_steps=1000):\n",
    "        self.reset_env(inc_counter=True)\n",
    "        was_game_finit = False\n",
    "        for _ in range(max_steps):\n",
    "            action = self.agent.get_single_action(\n",
    "                self.env_state,\n",
    "                need_argmax=False,\n",
    "                temperature=1,\n",
    "            )\n",
    "            self.env_state, reward, done, info = self.env.step(np.argmax(action))\n",
    "            \n",
    "            yield self.env_state, action, reward, done\n",
    "            \n",
    "            if done:\n",
    "                was_game_finit = True\n",
    "                break\n",
    "        return None, None, was_game_finit\n",
    "            \n",
    "    def get_test_game_total_reward(\n",
    "            self, \n",
    "            max_steps=1000, \n",
    "            temperature=10,\n",
    "            add_to_memory=True,\n",
    "    ):\n",
    "        total_reward = 0\n",
    "        was_game_finit = False\n",
    "        \n",
    "        for _, _, reward, done in self.iterate_over_test_game(max_steps=1000):\n",
    "            if done:\n",
    "                break\n",
    "            total_reward += reward\n",
    "\n",
    "            \n",
    "        if add_to_memory:\n",
    "            self.history.append([self.game_count, total_reward])\n",
    "        \n",
    "        return total_reward\n",
    "    \n",
    "    \n",
    "    def get_test_game_mean_reward(\n",
    "        self,\n",
    "        n_games=10,\n",
    "        max_steps=1000, \n",
    "        temperature=10,\n",
    "        add_to_memory=True\n",
    "    ):\n",
    "        sm = 0\n",
    "        for _ in range(n_games):\n",
    "            sm += self.get_test_game_total_reward(max_steps, temperature, add_to_memory=False)\n",
    "        sm /= n_games\n",
    "        \n",
    "        if add_to_memory:            \n",
    "            self.history.append([self.game_count, sm])\n",
    "            \n",
    "        return sm\n",
    "    \n",
    "    def get_history(self):\n",
    "        return np.array(self.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "holder = Holder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "holder.insert_N_sample_to_replay_memory(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "holder.update_agent(update_step_num=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
